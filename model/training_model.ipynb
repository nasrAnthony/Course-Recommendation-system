{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f601b481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import pos_tag, word_tokenize\n",
    "import nltk\n",
    "\n",
    "for pkg in ['punkt', 'punkt_tab', 'averaged_perceptron_tagger', 'wordnet', 'omw-1.4']:\n",
    "    nltk.download(pkg, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "421e34b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = \"../data/cleaned_courses.csv\"  # adjust if needed\n",
    "TEXT_COLUMN = \"TextForBERT\"\n",
    "\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "MAX_LEN = 64\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "LR = 2e-5\n",
    "PROJ_DIM = 256\n",
    "TEMPERATURE = 0.05\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae1bf483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17484780",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# Text Augmentations\n",
    "#   - random deletion\n",
    "#   - synonym replacement\n",
    "#   - random insertion\n",
    "#   - random swap\n",
    "# =========================\n",
    "\n",
    "def get_wordnet_pos(treebank_tag: str):\n",
    "    \"\"\"\n",
    "    Map NLTK POS tags to WordNet POS tags.\n",
    "    We mostly care about adjectives for your use case.\n",
    "    \"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_synonym(word: str, wn_pos=None):\n",
    "    \"\"\"Get a random synonym for a word (if available).\"\"\"\n",
    "    try:\n",
    "        synsets = wn.synsets(word, pos=wn_pos) if wn_pos else wn.synsets(word)\n",
    "        if not synsets:\n",
    "            return None\n",
    "\n",
    "        synset = random.choice(synsets)\n",
    "        lemmas = [l.name().replace('_', ' ') for l in synset.lemmas() if l.name().lower() != word.lower()]\n",
    "        if not lemmas:\n",
    "            return None\n",
    "\n",
    "        return random.choice(lemmas)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def random_deletion(words: List[str], p: float = 0.1) -> List[str]:\n",
    "    if len(words) == 1:\n",
    "        return words\n",
    "    kept = [w for w in words if random.random() > p]\n",
    "    if not kept:\n",
    "        kept = [random.choice(words)]\n",
    "    return kept\n",
    "\n",
    "\n",
    "def synonym_replacement(words: List[str], n: int = 1) -> List[str]:\n",
    "    \"\"\"Replace up to n words with synonyms (prefer adjectives).\"\"\"\n",
    "    if len(words) == 0:\n",
    "        return words\n",
    "\n",
    "    new_words = words.copy()\n",
    "    # POS tag to find adjectives etc.\n",
    "    tagged = pos_tag(new_words)\n",
    "    candidates = list(range(len(new_words)))  # indices\n",
    "\n",
    "    random.shuffle(candidates)\n",
    "    num_replaced = 0\n",
    "\n",
    "    for idx in candidates:\n",
    "        if num_replaced >= n:\n",
    "            break\n",
    "\n",
    "        word = new_words[idx]\n",
    "        if not word.isalpha():\n",
    "            continue\n",
    "\n",
    "        _, tag = tagged[idx]\n",
    "        wn_pos = get_wordnet_pos(tag)\n",
    "\n",
    "        # Prefer adjectives but allow others\n",
    "        synonym = get_synonym(word, wn_pos=wn_pos)\n",
    "        if synonym is None:\n",
    "            continue\n",
    "\n",
    "        new_words[idx] = synonym\n",
    "        num_replaced += 1\n",
    "\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def random_swap(words: List[str], n: int = 1) -> List[str]:\n",
    "    if len(words) < 2:\n",
    "        return words\n",
    "    new_words = words.copy()\n",
    "    for _ in range(n):\n",
    "        idx1, idx2 = random.sample(range(len(new_words)), 2)\n",
    "        new_words[idx1], new_words[idx2] = new_words[idx2], new_words[idx1]\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def random_insertion(words: List[str], n: int = 1) -> List[str]:\n",
    "    \"\"\"Insert n synonyms of random words at random positions.\"\"\"\n",
    "    new_words = words.copy()\n",
    "    length = len(new_words)\n",
    "    if length == 0:\n",
    "        return new_words\n",
    "\n",
    "    for _ in range(n):\n",
    "        idx = random.randrange(length)\n",
    "        word = new_words[idx]\n",
    "        if not word.isalpha():\n",
    "            continue\n",
    "\n",
    "        # Try to get synonym (any POS)\n",
    "        synonym = get_synonym(word)\n",
    "        if synonym is None:\n",
    "            continue\n",
    "\n",
    "        insert_pos = random.randrange(len(new_words) + 1)\n",
    "        new_words.insert(insert_pos, synonym)\n",
    "\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def augment_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Create one augmented view.\n",
    "    We apply a couple of augmentations with some probability,\n",
    "    trying not to completely destroy semantics.\n",
    "    \"\"\"\n",
    "    # Tokenize by words (nltk tokenizer is a bit smarter than split)\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    if len(words) == 0:\n",
    "        return text\n",
    "\n",
    "    # 1. Random deletion\n",
    "    if random.random() < 0.8:\n",
    "        words = random_deletion(words, p=0.1)\n",
    "\n",
    "    # 2. Synonym replacement (1–2 words)\n",
    "    if random.random() < 0.6:\n",
    "        words = synonym_replacement(words, n=random.choice([1, 2]))\n",
    "\n",
    "    # 3. Random insertion (0–2)\n",
    "    if random.random() < 0.4:\n",
    "        words = random_insertion(words, n=random.choice([1, 2]))\n",
    "\n",
    "    # 4. Random swap (once)\n",
    "    if random.random() < 0.3:\n",
    "        words = random_swap(words, n=1)\n",
    "\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "def make_two_views(text: str) -> Tuple[str, str]:\n",
    "    \"\"\"Return 2 independently augmented views of the same base text.\"\"\"\n",
    "    view1 = augment_text(text)\n",
    "    view2 = augment_text(text)\n",
    "    return view1, view2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65b86282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Dataset\n",
    "# =========================\n",
    "\n",
    "class ContrastiveCourseDataset(Dataset):\n",
    "    def __init__(self, texts: List[str], tokenizer, max_len: int):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        base_text = self.texts[idx]\n",
    "        view1, view2 = make_two_views(base_text)\n",
    "\n",
    "        encoded1 = self.tokenizer(\n",
    "            view1,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        encoded2 = self.tokenizer(\n",
    "            view2,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        item = {\n",
    "            \"input_ids_a\": encoded1[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask_a\": encoded1[\"attention_mask\"].squeeze(0),\n",
    "            \"input_ids_b\": encoded2[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask_b\": encoded2[\"attention_mask\"].squeeze(0),\n",
    "        }\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12bece9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Model: BERT + Projection Head\n",
    "# =========================\n",
    "\n",
    "class CourseEncoder(nn.Module):\n",
    "    def __init__(self, base_model_name: str = MODEL_NAME, proj_dim: int = PROJ_DIM):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(base_model_name)\n",
    "        hidden = self.bert.config.hidden_size\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, proj_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # CLS pooling\n",
    "        cls = out.last_hidden_state[:, 0, :]  # (batch, hidden)\n",
    "        z = self.proj(cls)\n",
    "        # Normalize for cosine similarity\n",
    "        z = nn.functional.normalize(z, p=2, dim=-1)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "19a9ee2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Contrastive Loss (InfoNCE / NT-Xent)\n",
    "# =========================\n",
    "\n",
    "class NTXentLoss(nn.Module):\n",
    "    def __init__(self, temperature: float = 0.05):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "        \"\"\"\n",
    "        z_i, z_j: (batch, dim)\n",
    "        Positive pairs are (i, i), negatives are all others in the batch.\n",
    "        \"\"\"\n",
    "        batch_size = z_i.size(0)\n",
    "\n",
    "        logits = torch.matmul(z_i, z_j.T) / self.temperature  # (N, N)\n",
    "        labels = torch.arange(batch_size, device=z_i.device)\n",
    "\n",
    "        loss_i = self.criterion(logits, labels)\n",
    "        loss_j = self.criterion(logits.T, labels)\n",
    "\n",
    "        loss = (loss_i + loss_j) / 2.0\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b7b47d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Training Loop\n",
    "# =========================\n",
    "\n",
    "def train():\n",
    "    # ---- Load data ----\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    if TEXT_COLUMN not in df.columns:\n",
    "        raise ValueError(f\"Column {TEXT_COLUMN} not found in CSV.\")\n",
    "\n",
    "    texts = df[TEXT_COLUMN].astype(str).tolist()\n",
    "    print(f\"Loaded {len(texts)} course descriptions.\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    dataset = ContrastiveCourseDataset(texts, tokenizer, MAX_LEN)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "    # ---- Model, optimizer, scheduler, loss ----\n",
    "    model = CourseEncoder().to(DEVICE)\n",
    "\n",
    "    # Optionally: freeze some lower BERT layers if you want\n",
    "    # for name, param in model.bert.named_parameters():\n",
    "    #     if \"encoder.layer.\" in name:\n",
    "    #         layer_num = int(name.split(\"encoder.layer.\")[-1].split(\".\")[0])\n",
    "    #         if layer_num < 8:  # freeze first 8 layers, for example\n",
    "    #             param.requires_grad = False\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=LR)\n",
    "    total_steps = len(dataloader) * EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(0.1 * total_steps),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    criterion = NTXentLoss(temperature=TEMPERATURE)\n",
    "\n",
    "    model.train()\n",
    "    step = 0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        for batch in dataloader:\n",
    "            step += 1\n",
    "            input_ids_a = batch[\"input_ids_a\"].to(DEVICE)\n",
    "            attention_mask_a = batch[\"attention_mask_a\"].to(DEVICE)\n",
    "            input_ids_b = batch[\"input_ids_b\"].to(DEVICE)\n",
    "            attention_mask_b = batch[\"attention_mask_b\"].to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            z_i = model(input_ids_a, attention_mask_a)  # (N, D)\n",
    "            z_j = model(input_ids_b, attention_mask_b)  # (N, D)\n",
    "\n",
    "            loss = criterion(z_i, z_j)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                avg_loss = running_loss / 10\n",
    "                print(f\"Epoch {epoch+1}/{EPOCHS} | Step {step} | Loss: {avg_loss:.4f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "    # ---- Save model ----\n",
    "    save_dir = \"course_encoder_contrastive\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Save both BERT + projection head\n",
    "    # Easiest is to save state_dict and tokenizer separately\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, \"pytorch_model.bin\"))\n",
    "    tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "    # Also save some config for reloading later\n",
    "    with open(os.path.join(save_dir, \"config.txt\"), \"w\") as f:\n",
    "        f.write(f\"MODEL_NAME={MODEL_NAME}\\n\")\n",
    "        f.write(f\"PROJ_DIM={PROJ_DIM}\\n\")\n",
    "        f.write(f\"MAX_LEN={MAX_LEN}\\n\")\n",
    "\n",
    "    print(f\"Model saved to {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf34475c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 527 course descriptions.\n",
      "Epoch 1/5 | Step 10 | Loss: 1.4313\n",
      "Epoch 1/5 | Step 20 | Loss: 0.3182\n",
      "Epoch 1/5 | Step 30 | Loss: 0.0502\n",
      "Epoch 2/5 | Step 40 | Loss: 0.0180\n",
      "Epoch 2/5 | Step 50 | Loss: 0.0098\n",
      "Epoch 2/5 | Step 60 | Loss: 0.0090\n",
      "Epoch 3/5 | Step 70 | Loss: 0.0028\n",
      "Epoch 3/5 | Step 80 | Loss: 0.0160\n",
      "Epoch 3/5 | Step 90 | Loss: 0.0022\n",
      "Epoch 4/5 | Step 100 | Loss: 0.0001\n",
      "Epoch 4/5 | Step 110 | Loss: 0.0041\n",
      "Epoch 4/5 | Step 120 | Loss: 0.0036\n",
      "Epoch 4/5 | Step 130 | Loss: 0.0019\n",
      "Epoch 5/5 | Step 140 | Loss: 0.0025\n",
      "Epoch 5/5 | Step 150 | Loss: 0.0170\n",
      "Epoch 5/5 | Step 160 | Loss: 0.0058\n",
      "Model saved to course_encoder_contrastive\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
