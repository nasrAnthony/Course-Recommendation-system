{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f601b481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import pos_tag, word_tokenize\n",
    "import nltk\n",
    "\n",
    "for pkg in ['punkt', 'punkt_tab', 'averaged_perceptron_tagger', 'wordnet', 'omw-1.4']:\n",
    "    nltk.download(pkg, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421e34b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = \"../data/cleaned_courses.csv\"  # adjust if needed\n",
    "CSV_PATH_AUGMENTED_DATA = \"../data/students_clean_train.csv\"\n",
    "TEXT_COLUMN = \"TextForBERT\"\n",
    "\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "MAX_LEN = 64\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "LR = 2e-5\n",
    "PROJ_DIM = 256\n",
    "TEMPERATURE = 0.075\n",
    "LAMBDA_ISO = 0.05  # weight for isotropy regularizer\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ae1bf483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "17484780",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# Text Augmentations\n",
    "#   - random deletion\n",
    "#   - synonym replacement\n",
    "#   - random insertion\n",
    "#   - random swap\n",
    "# =========================\n",
    "\n",
    "df_2 = pd.read_csv(CSV_PATH_AUGMENTED_DATA)\n",
    "\n",
    "def get_wordnet_pos(treebank_tag: str):\n",
    "    \"\"\"\n",
    "    Map NLTK POS tags to WordNet POS tags.\n",
    "    We mostly care about adjectives for your use case.\n",
    "    \"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_synonym(word: str, wn_pos=None):\n",
    "    \"\"\"Get a random synonym for a word (if available).\"\"\"\n",
    "    try:\n",
    "        synsets = wn.synsets(word, pos=wn_pos) if wn_pos else wn.synsets(word)\n",
    "        if not synsets:\n",
    "            return None\n",
    "\n",
    "        synset = random.choice(synsets)\n",
    "        lemmas = [l.name().replace('_', ' ') for l in synset.lemmas() if l.name().lower() != word.lower()]\n",
    "        if not lemmas:\n",
    "            return None\n",
    "\n",
    "        return random.choice(lemmas)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def random_deletion(words: List[str], p: float = 0.1) -> List[str]:\n",
    "    if len(words) == 1:\n",
    "        return words\n",
    "    kept = [w for w in words if random.random() > p]\n",
    "    if not kept:\n",
    "        kept = [random.choice(words)]\n",
    "    return kept\n",
    "\n",
    "\n",
    "def synonym_replacement(words: List[str], n: int = 1) -> List[str]:\n",
    "    \"\"\"Replace up to n words with synonyms (prefer adjectives).\"\"\"\n",
    "    if len(words) == 0:\n",
    "        return words\n",
    "\n",
    "    new_words = words.copy()\n",
    "    # POS tag to find adjectives etc.\n",
    "    tagged = pos_tag(new_words)\n",
    "    candidates = list(range(len(new_words)))  # indices\n",
    "\n",
    "    random.shuffle(candidates)\n",
    "    num_replaced = 0\n",
    "\n",
    "    for idx in candidates:\n",
    "        if num_replaced >= n:\n",
    "            break\n",
    "\n",
    "        word = new_words[idx]\n",
    "        if not word.isalpha():\n",
    "            continue\n",
    "\n",
    "        _, tag = tagged[idx]\n",
    "        wn_pos = get_wordnet_pos(tag)\n",
    "\n",
    "        # Prefer adjectives but allow others\n",
    "        synonym = get_synonym(word, wn_pos=wn_pos)\n",
    "        if synonym is None:\n",
    "            continue\n",
    "\n",
    "        new_words[idx] = synonym\n",
    "        num_replaced += 1\n",
    "\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def random_swap(words: List[str], n: int = 1) -> List[str]:\n",
    "    if len(words) < 2:\n",
    "        return words\n",
    "    new_words = words.copy()\n",
    "    for _ in range(n):\n",
    "        idx1, idx2 = random.sample(range(len(new_words)), 2)\n",
    "        new_words[idx1], new_words[idx2] = new_words[idx2], new_words[idx1]\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def random_insertion(words: List[str], n: int = 1) -> List[str]:\n",
    "    \"\"\"Insert n synonyms of random words at random positions.\"\"\"\n",
    "    new_words = words.copy()\n",
    "    length = len(new_words)\n",
    "    if length == 0:\n",
    "        return new_words\n",
    "\n",
    "    for _ in range(n):\n",
    "        idx = random.randrange(length)\n",
    "        word = new_words[idx]\n",
    "        if not word.isalpha():\n",
    "            continue\n",
    "\n",
    "        # Try to get synonym (any POS)\n",
    "        synonym = get_synonym(word)\n",
    "        if synonym is None:\n",
    "            continue\n",
    "\n",
    "        insert_pos = random.randrange(len(new_words) + 1)\n",
    "        new_words.insert(insert_pos, synonym)\n",
    "\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def augment_text(text: str, mode: str = \"light\") -> str:\n",
    "    \"\"\"\n",
    "    Create one augmented view.\n",
    "    mode = \"light\" or \"heavy\"\n",
    "\n",
    "    light  -> small changes, stays very close to original\n",
    "    heavy  -> larger perturbations, but try to keep semantics\n",
    "    \"\"\"\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    if len(words) == 0:\n",
    "        return text\n",
    "\n",
    "    # ---- Hyperparams by mode ----\n",
    "    if mode == \"light\":\n",
    "        del_prob = 0.05          # lower deletion prob\n",
    "        syn_prob = 0.4           # fewer synonym ops\n",
    "        syn_n_choices = [1]      # replace at most 1 word\n",
    "        ins_prob = 0.2           # rarely insert\n",
    "        ins_n_choices = [1]\n",
    "        swap_prob = 0.2\n",
    "    elif mode == \"heavy\":\n",
    "        del_prob = 0.15          # more deletion\n",
    "        syn_prob = 0.8           # more synonym operations\n",
    "        syn_n_choices = [1, 2, 3]\n",
    "        ins_prob = 0.5           # more insertion\n",
    "        ins_n_choices = [1, 2]\n",
    "        swap_prob = 0.4\n",
    "    else:\n",
    "        # fallback to light if unknown\n",
    "        del_prob = 0.05\n",
    "        syn_prob = 0.4\n",
    "        syn_n_choices = [1]\n",
    "        ins_prob = 0.2\n",
    "        ins_n_choices = [1]\n",
    "        swap_prob = 0.2\n",
    "\n",
    "    # 1. Random deletion\n",
    "    if random.random() < 0.8:  # keep same probability of *using* deletion\n",
    "        words = random_deletion(words, p=del_prob)\n",
    "\n",
    "    # 2. Synonym replacement\n",
    "    if random.random() < syn_prob:\n",
    "        words = synonym_replacement(words, n=random.choice(syn_n_choices))\n",
    "\n",
    "    # 3. Random insertion\n",
    "    if random.random() < ins_prob:\n",
    "        words = random_insertion(words, n=random.choice(ins_n_choices))\n",
    "\n",
    "    # 4. Random swap\n",
    "    if random.random() < swap_prob:\n",
    "        words = random_swap(words, n=1)\n",
    "\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "def make_two_views(text: str, course_code:str) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Return 2 differently augmented views of the same base text.\n",
    "\n",
    "    view1: light augmentation (close to original)\n",
    "    view2: heavy augmentation (more aggressively perturbed)\n",
    "    \"\"\"\n",
    "    rows = df_2.loc[df_2[\"LikedCourses\"].str.strip() == course_code, \"StudentText\"]\n",
    "    if not rows.empty:\n",
    "        view1 = rows.iloc[0]\n",
    "    else:\n",
    "        view1 = augment_text(text, mode=\"light\")\n",
    "    view2 = augment_text(text, mode=\"heavy\")\n",
    "    return view1, view2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "65b86282",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveCourseDataset(Dataset):\n",
    "    def __init__(self, texts: List[str], labels: List[int], tokenizer, full_codes: List[str], max_len: int):\n",
    "        \"\"\"\n",
    "        texts:  list of course descriptions (len = N)\n",
    "        labels: list of int faculty IDs aligned with texts (len = N)\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.course_codes = full_codes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        base_text = self.texts[idx]\n",
    "        view1, view2 = make_two_views(base_text, self.course_codes[idx])\n",
    "\n",
    "        encoded1 = self.tokenizer(\n",
    "            view1,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        encoded2 = self.tokenizer(\n",
    "            view2,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        item = {\n",
    "            \"input_ids_a\":      encoded1[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask_a\": encoded1[\"attention_mask\"].squeeze(0),\n",
    "            \"input_ids_b\":      encoded2[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask_b\": encoded2[\"attention_mask\"].squeeze(0),\n",
    "            \"label\":            torch.tensor(self.labels[idx], dtype=torch.long),\n",
    "        }\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "12bece9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Model: BERT + Projection Head\n",
    "# =========================\n",
    "\n",
    "class CourseEncoder(nn.Module):\n",
    "    def __init__(self, base_model_name: str = MODEL_NAME, proj_dim: int = PROJ_DIM):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(base_model_name)\n",
    "        hidden = self.bert.config.hidden_size\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, proj_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=False\n",
    "        )\n",
    "        last_hidden = out.last_hidden_state  # (B, L, H)\n",
    "\n",
    "        # mean pooling\n",
    "        mask = attention_mask.unsqueeze(-1).float()  # (B, L, 1)\n",
    "        masked = last_hidden * mask\n",
    "        summed = masked.sum(dim=1)\n",
    "        counts = mask.sum(dim=1).clamp(min=1e-9)\n",
    "        pooled = summed / counts                    # (B, H)\n",
    "\n",
    "        z = self.proj(pooled)\n",
    "        z = nn.functional.normalize(z, p=2, dim=-1)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "19a9ee2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Contrastive Loss (InfoNCE / NT-Xent)\n",
    "# =========================\n",
    "\n",
    "class SupervisedNTXentLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-positive supervised contrastive loss.\n",
    "\n",
    "    - Uses two views z_i, z_j: each (B, D), already L2-normalized.\n",
    "    - labels: (B,) integer labels (e.g., faculty IDs).\n",
    "    - Positives for each anchor = all embeddings in the batch (both views)\n",
    "      that share the same label (excluding itself).\n",
    "    \"\"\"\n",
    "    def __init__(self, temperature: float = 0.05):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, z_i, z_j, labels):\n",
    "        device = z_i.device\n",
    "        batch_size = z_i.size(0)\n",
    "\n",
    "        # Stack the two views: (2B, D)\n",
    "        z = torch.cat([z_i, z_j], dim=0)\n",
    "        z = nn.functional.normalize(z, p=2, dim=-1)\n",
    "\n",
    "        # Duplicate labels for both views: (2B,)\n",
    "        labels = labels.to(device)\n",
    "        labels_all = torch.cat([labels, labels], dim=0)\n",
    "\n",
    "        # Similarity matrix: (2B, 2B)\n",
    "        sim = torch.matmul(z, z.T) / self.temperature\n",
    "\n",
    "        # Mask to remove self-similarity\n",
    "        self_mask = torch.eye(2 * batch_size, dtype=torch.bool, device=device)\n",
    "\n",
    "        # Label equality mask: (2B, 2B)\n",
    "        label_eq = labels_all.unsqueeze(0) == labels_all.unsqueeze(1)\n",
    "        # Positives: same label, not self\n",
    "        positive_mask = label_eq & (~self_mask)\n",
    "\n",
    "        # For numerical stability\n",
    "        sim_max, _ = sim.max(dim=1, keepdim=True)\n",
    "        sim = sim - sim_max.detach()\n",
    "\n",
    "        exp_sim = torch.exp(sim)\n",
    "\n",
    "        # Denominator: sum over all j != i\n",
    "        exp_sim = exp_sim.masked_fill(self_mask, 0.0)\n",
    "        denom = exp_sim.sum(dim=1) + 1e-12\n",
    "\n",
    "        # Numerator: sum over positives\n",
    "        pos_exp = exp_sim * positive_mask.float()\n",
    "        numer = pos_exp.sum(dim=1) + 1e-12\n",
    "\n",
    "        # Only anchors that have at least one positive\n",
    "        valid = positive_mask.sum(dim=1) > 0\n",
    "        loss = -torch.log(numer / denom)\n",
    "        loss = loss[valid].mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    \n",
    "# =========================\n",
    "# Isotropy regularizer  <<< NEW\n",
    "# =========================\n",
    "def isotropy_regularizer(z: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    z: (batch, dim), assumed already L2-normalized along dim=-1.\n",
    "\n",
    "    Encourages:\n",
    "      - feature-wise mean ≈ 0\n",
    "      - feature-wise variance ≈ 1\n",
    "    \"\"\"\n",
    "    # feature means across the batch\n",
    "    mean = z.mean(dim=0)                    # (dim,)\n",
    "    # feature variances across the batch\n",
    "    var = z.var(dim=0, unbiased=False)      # (dim,)\n",
    "\n",
    "    mean_loss = (mean ** 2).mean()          # want mean -> 0\n",
    "    var_loss  = ((var - 1.0) ** 2).mean()   # want var -> 1\n",
    "\n",
    "    return mean_loss + var_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9b7b47d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Training Loop\n",
    "# =========================\n",
    "\n",
    "def train():\n",
    "    # ---- Load data ----\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    if TEXT_COLUMN not in df.columns:\n",
    "        raise ValueError(f\"Column {TEXT_COLUMN} not found in CSV.\")\n",
    "\n",
    "    texts = df[TEXT_COLUMN].astype(str).tolist()\n",
    "\n",
    "    print(f\"Loaded {len(texts)} course descriptions.\")\n",
    "\n",
    "    # ---- NEW: build faculty labels ----\n",
    "    if \"Faculty\" not in df.columns:\n",
    "        raise ValueError(\"Expected a 'Faculty' column in the CSV for labels.\")\n",
    "\n",
    "    faculties = df[\"Faculty\"].astype(str).tolist()\n",
    "    full_codes = df[\"Faculty\"].astype(str) + \" \" + df[\"Code\"].astype(str)\n",
    "    unique_faculties = sorted(set(faculties))\n",
    "    fac2id = {fac: i for i, fac in enumerate(unique_faculties)}\n",
    "    labels = [fac2id[f] for f in faculties]    # list[int], same length as texts\n",
    "\n",
    "    print(f\"Found {len(unique_faculties)} unique faculties.\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    dataset = ContrastiveCourseDataset(texts, labels, tokenizer, full_codes, MAX_LEN)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "    # ---- Model, optimizer, scheduler, loss ----\n",
    "    model = CourseEncoder().to(DEVICE)\n",
    "\n",
    "    # Optionally: freeze some lower BERT layers if you want\n",
    "    # for name, param in model.bert.named_parameters():\n",
    "    #     if \"encoder.layer.\" in name:\n",
    "    #         layer_num = int(name.split(\"encoder.layer.\")[-1].split(\".\")[0])\n",
    "    #         if layer_num < 8:  # freeze first 8 layers, for example\n",
    "    #             param.requires_grad = False\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=LR)\n",
    "    total_steps = len(dataloader) * EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(0.1 * total_steps),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    criterion = SupervisedNTXentLoss(temperature=TEMPERATURE)\n",
    "\n",
    "    model.train()\n",
    "    step = 0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        running_ctr = 0.0      # for logging\n",
    "        running_iso = 0.0      # for logging\n",
    "\n",
    "        for batch in dataloader:\n",
    "            step += 1\n",
    "            input_ids_a      = batch[\"input_ids_a\"].to(DEVICE)\n",
    "            attention_mask_a = batch[\"attention_mask_a\"].to(DEVICE)\n",
    "            input_ids_b      = batch[\"input_ids_b\"].to(DEVICE)\n",
    "            attention_mask_b = batch[\"attention_mask_b\"].to(DEVICE)\n",
    "            labels_batch     = batch[\"label\"].to(DEVICE)   # <<< NEW\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            z_i = model(input_ids_a, attention_mask_a)  # (N, D) normalized\n",
    "            z_j = model(input_ids_b, attention_mask_b)  # (N, D) normalized\n",
    "\n",
    "            # 1) supervised contrastive loss (multi-positive by faculty)\n",
    "            loss_contrastive = criterion(z_i, z_j, labels_batch)\n",
    "\n",
    "            # 2) isotropy regularizer on all embeddings in the batch\n",
    "            z_all = torch.cat([z_i, z_j], dim=0)        # (2N, D)\n",
    "            loss_iso = isotropy_regularizer(z_all)\n",
    "\n",
    "            # 3) total loss\n",
    "            loss = loss_contrastive + LAMBDA_ISO * loss_iso\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            running_ctr  += loss_contrastive.item()\n",
    "            running_iso  += loss_iso.item()\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                avg_loss = running_loss / 10\n",
    "                avg_ctr  = running_ctr  / 10\n",
    "                avg_iso  = running_iso  / 10\n",
    "                print(\n",
    "                    f\"Epoch {epoch+1}/{EPOCHS} | Step {step} | \"\n",
    "                    f\"Loss: {avg_loss:.4f} \"\n",
    "                    f\"(ctr: {avg_ctr:.4f}, iso: {avg_iso:.4f})\"\n",
    "                )\n",
    "                running_loss = 0.0\n",
    "                running_ctr  = 0.0\n",
    "                running_iso  = 0.0\n",
    "\n",
    "    # ---- Save model ----\n",
    "    save_dir = \"model_v6_temp_0.75\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Save both BERT + projection head\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, \"pytorch_model.bin\"))\n",
    "    tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "    # Also save some config for reloading later\n",
    "    with open(os.path.join(save_dir, \"config.txt\"), \"w\") as f:\n",
    "        f.write(f\"MODEL_NAME={MODEL_NAME}\\n\")\n",
    "        f.write(f\"PROJ_DIM={PROJ_DIM}\\n\")\n",
    "        f.write(f\"MAX_LEN={MAX_LEN}\\n\")\n",
    "\n",
    "    print(f\"Model saved to {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bf34475c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 518 course descriptions.\n",
      "Found 7 unique faculties.\n",
      "Epoch 1/5 | Step 10 | Loss: 1.1491 (ctr: 1.0990, iso: 1.0012)\n",
      "Epoch 1/5 | Step 20 | Loss: 0.3983 (ctr: 0.3485, iso: 0.9976)\n",
      "Epoch 1/5 | Step 30 | Loss: 0.2067 (ctr: 0.1569, iso: 0.9953)\n",
      "Epoch 2/5 | Step 40 | Loss: 0.1270 (ctr: 0.0922, iso: 0.6962)\n",
      "Epoch 2/5 | Step 50 | Loss: 0.1592 (ctr: 0.1095, iso: 0.9943)\n",
      "Epoch 2/5 | Step 60 | Loss: 0.1160 (ctr: 0.0663, iso: 0.9940)\n",
      "Epoch 3/5 | Step 70 | Loss: 0.0262 (ctr: 0.0064, iso: 0.3975)\n",
      "Epoch 3/5 | Step 80 | Loss: 0.1051 (ctr: 0.0554, iso: 0.9935)\n",
      "Epoch 3/5 | Step 90 | Loss: 0.0723 (ctr: 0.0227, iso: 0.9935)\n",
      "Epoch 4/5 | Step 100 | Loss: 0.0052 (ctr: 0.0002, iso: 0.0993)\n",
      "Epoch 4/5 | Step 110 | Loss: 0.0925 (ctr: 0.0428, iso: 0.9934)\n",
      "Epoch 4/5 | Step 120 | Loss: 0.0729 (ctr: 0.0232, iso: 0.9934)\n",
      "Epoch 4/5 | Step 130 | Loss: 0.0663 (ctr: 0.0166, iso: 0.9933)\n",
      "Epoch 5/5 | Step 140 | Loss: 0.0580 (ctr: 0.0183, iso: 0.7946)\n",
      "Epoch 5/5 | Step 150 | Loss: 0.0739 (ctr: 0.0243, iso: 0.9933)\n",
      "Epoch 5/5 | Step 160 | Loss: 0.0532 (ctr: 0.0036, iso: 0.9933)\n",
      "Model saved to model_v6_temp_0.75\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
